#!/usr/bin/env python3
"""
SteadyDancer ä½¿ç”¨ç¤ºä¾‹

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡ Python API è°ƒç”¨ SteadyDancer ç”Ÿæˆè§†é¢‘ã€‚

ä½¿ç”¨æ–¹æ³•:
    python examples/steadydancer_example.py \
        --image_start path/to/reference_image.jpg \
        --video_guide path/to/control_video.mp4 \
        --prompt "a person dancing" \
        --output output_video.mp4
"""

import argparse
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
from PIL import Image
import numpy as np
from shared.utils.utils import convert_image_to_tensor, save_video


def load_video_frames(video_path, max_frames=None):
    """
    åŠ è½½è§†é¢‘å¸§
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        max_frames: æœ€å¤§å¸§æ•°ï¼ˆNone è¡¨ç¤ºåŠ è½½æ‰€æœ‰å¸§ï¼‰
    
    Returns:
        torch.Tensor: å½¢çŠ¶ä¸º (C, T, H, W) çš„è§†é¢‘å¼ é‡ï¼Œå€¼èŒƒå›´ [-1, 1]
    """
    try:
        import cv2
    except ImportError:
        raise ImportError("éœ€è¦å®‰è£… opencv-python: pip install opencv-python")
    
    cap = cv2.VideoCapture(video_path)
    frames = []
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if max_frames and len(frames) >= max_frames:
            break
        
        # è½¬æ¢ä¸º RGB
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        # è½¬æ¢ä¸º PIL Image ç„¶ååˆ° tensor
        frame_pil = Image.fromarray(frame)
        frame_tensor = convert_image_to_tensor(frame_pil)
        frames.append(frame_tensor)
    
    cap.release()
    
    if not frames:
        raise ValueError(f"æ— æ³•ä»è§†é¢‘ä¸­è¯»å–å¸§: {video_path}")
    
    # å †å ä¸º (C, T, H, W)
    video_tensor = torch.stack(frames, dim=1)
    return video_tensor


def generate_steadydancer_video(
    image_start_path,
    video_guide_path,
    prompt,
    output_path,
    video_mask_path=None,
    negative_prompt="",
    resolution=(480, 832),
    video_length=81,
    seed=42,
    sampling_steps=50,
    guidance_scale=5.0,
    alt_guidance_scale=2.0,
    device="cuda" if torch.cuda.is_available() else "cpu",
):
    """
    ä½¿ç”¨ SteadyDancer ç”Ÿæˆè§†é¢‘
    
    Args:
        image_start_path: å‚è€ƒå›¾åƒè·¯å¾„
        video_guide_path: æ§åˆ¶è§†é¢‘è·¯å¾„ï¼ˆåŒ…å«å§¿æ€åŠ¨ä½œï¼‰
        prompt: æ–‡æœ¬æç¤ºè¯
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        video_mask_path: å¯é€‰çš„è§†é¢‘æ©ç è·¯å¾„
        negative_prompt: è´Ÿé¢æç¤ºè¯
        resolution: åˆ†è¾¨ç‡ (width, height)
        video_length: è§†é¢‘é•¿åº¦ï¼ˆå¸§æ•°ï¼‰
        seed: éšæœºç§å­
        sampling_steps: é‡‡æ ·æ­¥æ•°
        guidance_scale: æ–‡æœ¬å¼•å¯¼å¼ºåº¦
        alt_guidance_scale: æ¡ä»¶å¼•å¯¼å¼ºåº¦ï¼ˆå§¿æ€å¼•å¯¼ï¼‰
        device: è®¡ç®—è®¾å¤‡
    """
    print(f"ğŸš€ å¼€å§‹ SteadyDancer è§†é¢‘ç”Ÿæˆ...")
    print(f"   å‚è€ƒå›¾åƒ: {image_start_path}")
    print(f"   æ§åˆ¶è§†é¢‘: {video_guide_path}")
    print(f"   æç¤ºè¯: {prompt}")
    print(f"   åˆ†è¾¨ç‡: {resolution[0]}x{resolution[1]}")
    print(f"   è§†é¢‘é•¿åº¦: {video_length} å¸§")
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    from models.wan import WanAny2V
    from models.wan.configs import WAN_CONFIGS
    from models.wan.wan_handler import family_handler
    
    cfg = WAN_CONFIGS['i2v-14B']
    model_filename = "wan2.1_steadydancer_14B_mbf16.safetensors"
    
    wan_model = WanAny2V(
        config=cfg,
        checkpoint_dir="ckpts",
        model_filename=model_filename,
        model_type="steadydancer",
        base_model_type="steadydancer",
        dtype=torch.bfloat16,
    )
    wan_model.model.to(device)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # åŠ è½½è¾“å…¥
    print("\nğŸ“‚ åŠ è½½è¾“å…¥æ–‡ä»¶...")
    image_start = Image.open(image_start_path).convert("RGB")
    image_start_tensor = convert_image_to_tensor(image_start).to(device)
    
    video_guide = load_video_frames(video_guide_path).to(device)
    print(f"   æ§åˆ¶è§†é¢‘å¸§æ•°: {video_guide.shape[1]}")
    
    video_mask = None
    if video_mask_path:
        video_mask = load_video_frames(video_mask_path).to(device)
        print(f"   è§†é¢‘æ©ç å¸§æ•°: {video_mask.shape[1]}")
    
    print("âœ… è¾“å…¥æ–‡ä»¶åŠ è½½å®Œæˆ")
    
    # ç”Ÿæˆè§†é¢‘
    print("\nğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘...")
    print(f"   é‡‡æ ·æ­¥æ•°: {sampling_steps}")
    print(f"   æ–‡æœ¬å¼•å¯¼: {guidance_scale}")
    print(f"   æ¡ä»¶å¼•å¯¼: {alt_guidance_scale}")
    
    with torch.no_grad():
        samples = wan_model.generate(
            input_prompt=prompt,
            n_prompt=negative_prompt,
            image_start=image_start_tensor,
            input_video=video_guide,
            video_mask=video_mask,
            height=resolution[1],
            width=resolution[0],
            frame_num=video_length,
            sampling_steps=sampling_steps,
            guide_scale=guidance_scale,
            alt_guide_scale=alt_guide_scale,
            seed=seed,
            video_prompt_type="VA" if video_mask else "V",
            image_prompt_type="S",
        )
    
    print("âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ")
    
    # ä¿å­˜è§†é¢‘
    print(f"\nğŸ’¾ ä¿å­˜è§†é¢‘åˆ°: {output_path}")
    save_video(samples, output_path, fps=16)
    print("âœ… è§†é¢‘ä¿å­˜å®Œæˆ")
    
    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="SteadyDancer è§†é¢‘ç”Ÿæˆç¤ºä¾‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºç¡€ç”¨æ³•
  python examples/steadydancer_example.py \\
      --image_start person.jpg \\
      --video_guide dance.mp4 \\
      --prompt "a person dancing gracefully" \\
      --output result.mp4

  # å¸¦æ©ç çš„ç²¾ç¡®æ§åˆ¶
  python examples/steadydancer_example.py \\
      --image_start person.jpg \\
      --video_guide dance.mp4 \\
      --video_mask mask.mp4 \\
      --prompt "a person dancing" \\
      --output result.mp4 \\
      --alt_guidance_scale 2.5
        """
    )
    
    parser.add_argument(
        "--image_start",
        type=str,
        required=True,
        help="å‚è€ƒå›¾åƒè·¯å¾„ï¼ˆåŒ…å«è¦åŠ¨ç”»åŒ–çš„äººç‰©ï¼‰"
    )
    parser.add_argument(
        "--video_guide",
        type=str,
        required=True,
        help="æ§åˆ¶è§†é¢‘è·¯å¾„ï¼ˆåŒ…å«å§¿æ€åŠ¨ä½œï¼‰"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        required=True,
        help="æ–‡æœ¬æç¤ºè¯"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="steadydancer_output.mp4",
        help="è¾“å‡ºè§†é¢‘è·¯å¾„ï¼ˆé»˜è®¤: steadydancer_output.mp4ï¼‰"
    )
    parser.add_argument(
        "--video_mask",
        type=str,
        default=None,
        help="å¯é€‰çš„è§†é¢‘æ©ç è·¯å¾„"
    )
    parser.add_argument(
        "--negative_prompt",
        type=str,
        default="",
        help="è´Ÿé¢æç¤ºè¯"
    )
    parser.add_argument(
        "--resolution",
        type=str,
        default="480x832",
        help="åˆ†è¾¨ç‡ï¼Œæ ¼å¼: WIDTHxHEIGHTï¼ˆé»˜è®¤: 480x832ï¼‰"
    )
    parser.add_argument(
        "--video_length",
        type=int,
        default=81,
        help="è§†é¢‘é•¿åº¦ï¼ˆå¸§æ•°ï¼Œé»˜è®¤: 81ï¼‰"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼ˆé»˜è®¤: 42ï¼‰"
    )
    parser.add_argument(
        "--sampling_steps",
        type=int,
        default=50,
        help="é‡‡æ ·æ­¥æ•°ï¼ˆé»˜è®¤: 50ï¼‰"
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=5.0,
        help="æ–‡æœ¬å¼•å¯¼å¼ºåº¦ï¼ˆé»˜è®¤: 5.0ï¼‰"
    )
    parser.add_argument(
        "--alt_guidance_scale",
        type=float,
        default=2.0,
        help="æ¡ä»¶å¼•å¯¼å¼ºåº¦/å§¿æ€å¼•å¯¼ï¼ˆé»˜è®¤: 2.0ï¼‰"
    )
    parser.add_argument(
        "--device",
        type=str,
        default=None,
        help="è®¡ç®—è®¾å¤‡ï¼ˆé»˜è®¤: è‡ªåŠ¨æ£€æµ‹ï¼‰"
    )
    
    args = parser.parse_args()
    
    # è§£æåˆ†è¾¨ç‡
    width, height = map(int, args.resolution.split('x'))
    resolution = (width, height)
    
    # ç¡®å®šè®¾å¤‡
    if args.device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cpu":
        print("âš ï¸  è­¦å‘Š: ä½¿ç”¨ CPU æ¨¡å¼ï¼Œç”Ÿæˆé€Ÿåº¦ä¼šå¾ˆæ…¢")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.image_start):
        print(f"âŒ é”™è¯¯: å‚è€ƒå›¾åƒä¸å­˜åœ¨: {args.image_start}")
        sys.exit(1)
    
    if not os.path.exists(args.video_guide):
        print(f"âŒ é”™è¯¯: æ§åˆ¶è§†é¢‘ä¸å­˜åœ¨: {args.video_guide}")
        sys.exit(1)
    
    if args.video_mask and not os.path.exists(args.video_mask):
        print(f"âŒ é”™è¯¯: è§†é¢‘æ©ç ä¸å­˜åœ¨: {args.video_mask}")
        sys.exit(1)
    
    # ç”Ÿæˆè§†é¢‘
    try:
        output_path = generate_steadydancer_video(
            image_start_path=args.image_start,
            video_guide_path=args.video_guide,
            prompt=args.prompt,
            output_path=args.output,
            video_mask_path=args.video_mask,
            negative_prompt=args.negative_prompt,
            resolution=resolution,
            video_length=args.video_length,
            seed=args.seed,
            sampling_steps=args.sampling_steps,
            guidance_scale=args.guidance_scale,
            alt_guidance_scale=args.alt_guidance_scale,
            device=device,
        )
        print(f"\nğŸ‰ æˆåŠŸï¼è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

